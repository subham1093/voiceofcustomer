{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Flow\n",
    "\n",
    "The processing flow is run on all of the review data once the models are all trained up and validated. This is the flow that will actually turn the raw data into processed output.\n",
    "\n",
    "On each step, you will find a \"Persistence Section\" which is concerned with storing the information for use in the following steps. For the purpose of this exercise, we are using a Cloudant database instance. If you decide to use a different storage type, the Persistence section is the one you will need to modify.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Analysis\n",
    "\n",
    "This step allows Alchemy to replace words in the sentences of the reviews by their semantic types (product, customer_service, company, etc); identify the sentiment of each sentence and the overall sentiment of a paragraph and identify the relationship between entities. \n",
    "\n",
    "The semantic types and relationship between entities were defined when the WKS model was trained and are usually associated with a given domain data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "import cloudant\n",
    "import nltk\n",
    "import utils\n",
    "import cloudanthelper as ch\n",
    "from watson_developer_cloud import AlchemyLanguageV1\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "logger.debug(config.sections())\n",
    "model_id = config['WKS']['WKS_MODEL_ID']\n",
    "alchemy_api = AlchemyLanguageV1(api_key = \n",
    "                    config['ALCHEMY']['ALCHEMY_API_KEY'])\n",
    "MAX_CHARS = 5024\n",
    "\n",
    "def get_entities(review):\n",
    "    \"\"\"\n",
    "    Get entities, along with sentiment, from Alchemy service.\n",
    "    Input: text which contains entities.\n",
    "    Output: json object with response from the service.\n",
    "    \"\"\"\n",
    "    logger.debug(review)\n",
    "    response = ''\n",
    "    try:\n",
    "        response = alchemy_api.entities(text=str(review), model=model_id, sentiment=True)\n",
    "    except:\n",
    "        logger.error(\"Error when getting entities.\")\n",
    "    logger.debug(\"Result from entities call: \"+str(response))\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_relations(review):\n",
    "    \"\"\"\n",
    "    Get relations from Alchemy service.\n",
    "    Input: text which contains relations between entities.\n",
    "    Output: json object with response from the service.\n",
    "    \"\"\"\n",
    "    split = {}\n",
    "    #splitting long excerpts to send to the service\n",
    "    if len(review) > MAX_CHARS:\n",
    "        mid = utils.find_middle(review)\n",
    "        while mid >= MAX_CHARS:\n",
    "            mid = utils.find_middle(review[:mid])\n",
    "        half = review[mid:]\n",
    "        review = review[:mid]\n",
    "        split = get_relations(half)\n",
    "    #calling the Alchemy service\n",
    "    try:\n",
    "        response = alchemy_api.typed_relations(text=review, model=model_id)\n",
    "        while response['status'] == 'ERROR':\n",
    "            if 'language' in response:\n",
    "                if response['language'] != 'english':\n",
    "                    break\n",
    "            time.sleep(60)\n",
    "        #if excerpt was too long and split, make sure they are merged\n",
    "        if split != {}:\n",
    "            if 'typedRelations' in response and 'typedRelations' in split:\n",
    "                response['typedRelations'] = response['typedRelations'] + split['typedRelations']\n",
    "                response['text'] = response['text'] + split['text']\n",
    "            elif 'typedRelations' in split and 'typedRelations' not in response:\n",
    "                response['typedRelations'] = split['typedRelations']\n",
    "        return response\n",
    "    except:\n",
    "        logger.error(\"Error when getting relations.\")\n",
    "    \n",
    "\n",
    "def token_replacement_entities(review_text):\n",
    "    \"\"\"\n",
    "    Replaces the identified tokens by their\n",
    "        semantic types.\n",
    "    Input: text to replace identified tokens.\n",
    "    Output: sentences with tokens replaced by their\n",
    "            semantic types.\n",
    "    \"\"\"\n",
    "    review = get_relations(review_text)\n",
    "    entity_info = get_entities(review_text)\n",
    "    entity_info = utils.avg_sentiment(entity_info)\n",
    "    entities = []\n",
    "    sentences = []\n",
    "    if 'entities' in entity_info:\n",
    "        entities = entity_info['entities']\n",
    "    try:\n",
    "        sentences = nltk.tokenize.sent_tokenize(review_text)\n",
    "    except:\n",
    "        logger.error(\"Error when splitting text into sentences.\")\n",
    "    result = []\n",
    "    sentence_dict = {}\n",
    "    seq_no = 0\n",
    "    i = seq_no\n",
    "    entry = {}\n",
    "    for sentence in sentences:\n",
    "        entry = {}\n",
    "        sentence_dict[sentence] = i - seq_no\n",
    "        entry['sentence'] = sentence\n",
    "        entry['seqno'] = i\n",
    "        i += 1\n",
    "        for entity in entities:\n",
    "            token = entity['text']\n",
    "            token = re.escape(token)\n",
    "            token = re.sub(r'\\\\ ', ' ', token)\n",
    "            if re.search(r'\\b%s\\b' % token, sentence) is not None:\n",
    "                test = {}\n",
    "                test['name'] = token\n",
    "                if 'sentiment' in entity:\n",
    "                    test['sentiment'] = [entity['sentiment']['type']]\n",
    "                if entity['type'] in entry:\n",
    "                    entry[entity['type']].append(test)\n",
    "                else:\n",
    "                    entry[entity['type']] = []\n",
    "                    entry[entity['type']].append(test)\n",
    "                count = int(entity['count'])\n",
    "                classification = \"<\" + entity['type'] + \">\"\n",
    "                sentence = re.sub(r'\\b%s\\b' % token, classification, sentence, count=count)\n",
    "                entry['replaced_sentence'] = sentence\n",
    "        result.append(entry)\n",
    "    #processing relationship between entities\n",
    "    if review is not None and 'typedRelations' in review:\n",
    "        types = review['typedRelations']\n",
    "        if types != []:\n",
    "            for text in types:\n",
    "                temp_dict = {}\n",
    "                temp_dict['hasrel'] = text['type']\n",
    "                temp_dict['rel_name'] = text['arguments'][0]['entities'][0]['text']\n",
    "                typed_entity = text['arguments'][0]['entities'][0]['type']\n",
    "                temp_dict['second'] = text['arguments'][1]['entities'][0]['text']\n",
    "                sentence = text['sentence']\n",
    "                if sentence in sentence_dict:\n",
    "                    entry = result[sentence_dict[sentence]]\n",
    "                    if typed_entity in entry:\n",
    "                        local = entry[typed_entity]\n",
    "                        local.append(temp_dict)\n",
    "                    else:\n",
    "                        local = [temp_dict]\n",
    "                        entry[typed_entity] = local\n",
    "\n",
    "    final_result = [result, i]\n",
    "    return final_result\n",
    "\n",
    "##---------------------------PERSISTENCE SECTION ------------------------------##\n",
    "\n",
    "#Initializing Cloudant client\n",
    "client = ch.getConnection()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "\n",
    "try:\n",
    "    status = db['tracker']\n",
    "except KeyError:\n",
    "    \"Tracker document not found.\"\n",
    "    \n",
    "#Making sure documents are not added multiple\n",
    "#times to the database\n",
    "for doc in db:\n",
    "    if 'type' in doc and doc['type'] == 'replaced':\n",
    "        doc.delete()\n",
    "status['replace_switch'] = 0\n",
    "status['replaced'] = []\n",
    "status.save()\n",
    "\n",
    "#Adding token replacement to an augmented version of the\n",
    "#document so that history can be preserved\n",
    "if utils.add_review(status) < 2:\n",
    "    status['replace_switch'] = 1\n",
    "    status.save()\n",
    "\n",
    "for doc in db:\n",
    "    if 'type' not in doc and doc['_id'] != 'tracker':\n",
    "        new_doc = {}\n",
    "        asin = doc['asin']\n",
    "        if doc['_id'] not in status['replaced']:\n",
    "            try:\n",
    "                review_id = doc['_id']\n",
    "                new_doc['review_id'] = review_id\n",
    "                text = doc['reviewText']\n",
    "                text = token_replacement_entities(text)\n",
    "                new_doc['review'] = text\n",
    "                new_doc['type'] = 'replaced'\n",
    "                new_doc['asin'] = asin\n",
    "                new_doc['helpful'] = doc['helpful']\n",
    "                new_doc['title'] = doc['title']\n",
    "                status['replaced'].append(review_id)\n",
    "                db.create_document(new_doc)\n",
    "                status.save()\n",
    "            except:\n",
    "                logger.error('Error saving token replaced document to Cloudant.')            \n",
    "status['classify_switch'] = 1\n",
    "status.save()\n",
    "\n",
    "client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating views and indexes\n",
    "\n",
    "NOTE: This step is required if:\n",
    "    1. You are using Cloudant to store your data\n",
    "    2. You want to execute each one of the following steps using Cloudant\n",
    "    \n",
    "To increase performance when using a Cloudant database, the use of views is recommended.\n",
    "\n",
    "For the purpose of this exercise, four views are used and they should be created on the Cloudant tool. This speeds up the retrieval of documents allowing the application to get the needed data with a lower number of calls to the Cloudant service.\n",
    "\n",
    "The views are:\n",
    "    _design/names/replaced\n",
    "    \n",
    "    _design/names/classified\n",
    "    \n",
    "    _design/names/clustered\n",
    "    \n",
    "    _design/names/final\n",
    "    \n",
    "Please refer to the [service tutorial](https://cloudant.com/learning-center/#getstarted) to learn how to create views. The map functions that you should enter when creating the views are the following:\n",
    "\n",
    "    - for replaced:\n",
    "        function (doc) \n",
    "          {if(doc.type == \"replaced\")\n",
    "            {emit(doc._id, doc);\n",
    "          }\n",
    "        }\n",
    "    - for classified:\n",
    "        function (doc) \n",
    "          {if(doc.type == \"classified\")\n",
    "            {emit(doc._id, doc);\n",
    "          }\n",
    "        }\n",
    "    - for clustered:\n",
    "        function (doc) \n",
    "          {if(doc.type == \"clustered\")\n",
    "            {emit(doc._id, doc);\n",
    "          }\n",
    "        }\n",
    "    - for final:\n",
    "        function (doc) \n",
    "          {if(doc.type == \"final\")\n",
    "            {emit(doc._id, doc);\n",
    "          }\n",
    "        }\n",
    "\n",
    "No reduce functions are needed.\n",
    "\n",
    "Some steps perform queries to the database. These are specific to the products needed to be retrieved, so no views are created for those. Instead, in order to query the documents, indices should be created in the database. The inidices can be created in the Query section of the tools and should be for the document fields:\n",
    "    \"type\"\n",
    "    \"asin\"\n",
    "    \"review_id\"\n",
    "    \n",
    "The content of the new index should look like:\n",
    "\n",
    "    {\n",
    "      \"index\": {\n",
    "        \"fields\": [\n",
    "          \"type\",\n",
    "          \"asin\",\n",
    "          \"review_id\"\n",
    "        ]\n",
    "      },\n",
    "      \"type\": \"json\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification of reviews\n",
    "\n",
    "This step uses the Natural Language Classifier (NLC) created on the Training notebook. It classifies a review and adds the result of the classification to a new document with type = 'classified' in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import configparser\n",
    "import cloudant\n",
    "import cloudanthelper as ch\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "NLC_USERNAME = config['NLC']['NLC_USERNAME']\n",
    "NLC_PASSWORD = config['NLC']['NLC_PASSWORD']\n",
    "NLC_CLASSIFIER = config['NLC']['NLC_CLASSIFIER']\n",
    "\n",
    "#initializing classifier object\n",
    "nlc = NaturalLanguageClassifierV1(username=NLC_USERNAME, \n",
    "                                  password=NLC_PASSWORD)\n",
    "\n",
    "def classify(review):\n",
    "    \"\"\"\n",
    "    Classifies a sentence based on the trained classifier.\n",
    "    Input: text to classify.\n",
    "    Output: top class returned by the service.\n",
    "    \"\"\"\n",
    "    logger.debug(review)\n",
    "    #Classify sentence\n",
    "    try:\n",
    "        response = nlc.classify(NLC_CLASSIFIER, review)\n",
    "        logger.debug(response)\n",
    "        return response['top_class']\n",
    "    except:\n",
    "        logger.error('Failed at sentence classification')\n",
    "        return 'no class'\n",
    "\n",
    "\n",
    "##---------------------------PERSISTENCE SECTION ------------------------------##\n",
    "\n",
    "#Initializing Cloudant client\n",
    "client = ch.getConnection()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "    \n",
    "#Making sure documents are not added multiple\n",
    "#times to the database\n",
    "try:\n",
    "    status = db['tracker']\n",
    "except KeyError:\n",
    "    \"Tracker document not found.\"\n",
    "    \n",
    "classified = ch.getResultsfromView(\"classified\", \"names\", db)\n",
    "for doc in classified.result:\n",
    "    doc = doc['value']\n",
    "    doc = ch.convertToDocument(db, doc['_id'])\n",
    "    doc.fetch()\n",
    "    doc.delete()\n",
    "status['cluster_switch'] = 0\n",
    "status['classified'] = []\n",
    "status.save()\n",
    "    \n",
    "if utils.add_review(status) == 2:\n",
    "    replaced = ch.getResultsfromView(\"replaced\", \"names\", db)\n",
    "    for doc in replaced.result:\n",
    "        doc = doc['value']\n",
    "        rev_id = doc['review_id']\n",
    "        if rev_id not in status['classified']:\n",
    "            doc_id = doc['_id']\n",
    "            del doc['_id']\n",
    "            new_doc['type'] = 'classified'\n",
    "            new_doc['review_id'] = doc['review_id']\n",
    "            new_doc['asin'] = doc['asin']\n",
    "            new_doc['review'] = doc['review']\n",
    "            new_doc['helpful'] = doc['helpful']\n",
    "            new_doc['title'] = doc['title']\n",
    "            for line in new_doc['review'][0]:\n",
    "                logger.debug(doc)\n",
    "                try:\n",
    "                    if('replaced_sentence' in line):\n",
    "                        sentence = line['replaced_sentence']\n",
    "                    else:\n",
    "                        sentence = line['sentence']\n",
    "                    if len(sentence) < 1024:\n",
    "                        line['layer1type'] = classify(sentence)\n",
    "                    else:\n",
    "                        line['layer1type'] = 'Sentence too long to Classify'\n",
    "                except:\n",
    "                    logger.error('Error classifying review.')\n",
    "            status['classified'].append(doc_id)\n",
    "            db.create_document(new_doc)\n",
    "            status.save()\n",
    "    status['cluster_switch'] = 1\n",
    "\n",
    "client.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering features\n",
    "\n",
    "This step allows features of a given product that are semantically similar to be grouped together. They are listed with the set of synonyms and the respective sentiment expressed by the reviewers when talking about them.\n",
    "\n",
    "Please not that this step is only required in order to have the data needed to populate the application presented in the demo. Feel free to skip this and the next steps if you already accomplished what you wanted with your data on the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cloudant\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import configparser\n",
    "import utils\n",
    "import cloudanthelper as ch\n",
    "from cloudant.query import Query\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "\n",
    "#Please provide the path to the word2vec model you created. \n",
    "#The path provided by default points to the available sample\n",
    "#model file\n",
    "W2V_MODEL = os.path.join(curdir,'..','data','sample_model.bin')\n",
    "\n",
    "\n",
    "def cluster(asin, doc, db):\n",
    "    \"\"\"\n",
    "    Cluster features based on their word vector similarity.\n",
    "    Input: document containing features and database connection.\n",
    "    Output: json object to be added to database document.\n",
    "    \"\"\"\n",
    "    query = Query(db, selector={'asin': asin, 'type':'replaced'},fields=[\"helpful\", \"title\", \"review_id\"])\n",
    "    name = query.result[0][0]['title']\n",
    "    rev_id = []\n",
    "    helpful={}\n",
    "    for data in query.result:\n",
    "        rev_id.append(data['review_id'])\n",
    "        if 'helpful' in data:\n",
    "            helpful[data['review_id']]=data['helpful'][0]\n",
    "        else:\n",
    "            helpful[data['review_id']]=0\n",
    "    temp = {}\n",
    "    keys = []\n",
    "    local_dump = {}\n",
    "    for rev in rev_id:\n",
    "        query_id = Query(db, selector={'review_id':rev, 'type':'classified'})\n",
    "        for i in query_id.result:\n",
    "            if len(query_id.result[0]) == 0:\n",
    "                continue\n",
    "        for res in query_id.result[0]:\n",
    "            text = res['review']\n",
    "            local_dump[res['review_id']] = text\n",
    "            for obj in text[0]:\n",
    "                if 'Feature' in obj:\n",
    "                    feature = obj['Feature']\n",
    "                    for data in feature:\n",
    "                        if 'name' in data:\n",
    "                            temp = {}\n",
    "                            temp['word'] = data['name']\n",
    "                            if 'sentiment' in data:\n",
    "                                temp['sentiment'] = data['sentiment']\n",
    "                            else:\n",
    "                                temp['sentiment'] = ['neutral']\n",
    "                            temp['review_id'] = res['review_id']\n",
    "                            temp['sentence_id'] = obj['seqno']\n",
    "                            keys.append(temp)\n",
    "\n",
    "    model = word2vec.Word2Vec.load_word2vec_format(W2V_MODEL, binary=True)\n",
    "    [vecs, mapping] = utils.generate_vectors(keys, model)\n",
    "    featureDict = {}\n",
    "    if len(vecs) > 0:\n",
    "        clusters = utils.cluster_try(vecs)\n",
    "        cluster_data = []\n",
    "        features = utils.create_json(clusters, cluster_data, mapping, keys, helpful, local_dump)\n",
    "        features = sorted(features, key=lambda k: k['keyword_count'], reverse=True)\n",
    "        featureDict['features'] = features[:10]\n",
    "    featureDict['product_name'] = name\n",
    "    return featureDict\n",
    "\n",
    "##---------------------------PERSISTENCE SECTION ------------------------------##\n",
    "\n",
    "#Initializing Cloudant client\n",
    "client = ch.getConnection()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "\n",
    "#Making sure documents are not added multiple\n",
    "#times to the database\n",
    "try:\n",
    "    status = db['tracker']\n",
    "except KeyError:\n",
    "    \"Tracker document not found.\"\n",
    "    \n",
    "clustered = ch.getResultsfromView(\"clustered\", \"names\", db)\n",
    "for doc in clustered.result:\n",
    "    doc = doc['value']\n",
    "    doc = ch.convertToDocument(db, doc['_id'])\n",
    "    doc.fetch()\n",
    "    doc.delete()\n",
    "status['final_switch'] = 0\n",
    "status['clustered'] = []\n",
    "status.save()\n",
    "\n",
    "if utils.add_review(status) == 3:\n",
    "    classified = ch.getResultsfromView(\"classified\", \"names\", db)\n",
    "    for doc in classified.result:\n",
    "        doc = doc['value']\n",
    "        rev_id = doc['review_id']\n",
    "        del doc['_id']\n",
    "        review = db[rev_id]\n",
    "        asin = review['asin']\n",
    "        if asin not in status['clustered']:\n",
    "            processed = cluster(asin, doc, db)\n",
    "            #processed['review_id'] = doc['review_id']\n",
    "            #processed['review'] = doc['review']\n",
    "            processed['product_id'] = asin\n",
    "            processed['type'] = 'clustered'\n",
    "            status['clustered'].append(asin)\n",
    "            db.create_document(processed)\n",
    "            status.save()\n",
    "    status['final_switch'] = 1\n",
    "\n",
    "client.disconnect()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Aggregating information\n",
    "\n",
    "This step creates a json with the compiled information about feature clustering. It will allow the sentences that are about specific features of a product to be grouped together and assess the sentiment associated with it.\n",
    "\n",
    "It compiles the sentences based on features that were identified as synonyms by the similarity between their word vectors and saves the information to the database with type = 'final'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import utils\n",
    "import logging\n",
    "import os\n",
    "import configparser\n",
    "import cloudanthelper as ch\n",
    "from cloudant.query import Query\n",
    "from cloudant.client import Cloudant\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "logger.debug(curdir)\n",
    "\n",
    "#loading credentials from .env file\n",
    "credFilePath = os.path.join(curdir,'..','.env')\n",
    "config = configparser.ConfigParser()\n",
    "config.read(credFilePath)\n",
    "\n",
    "##---------------------------PERSISTENCE SECTION ------------------------------##\n",
    "\n",
    "#Initializing Cloudant client\n",
    "client = ch.getConnection()\n",
    "db = client[config['CLOUDANT']['CLOUDANT_DB']]\n",
    "\n",
    "#Making sure documents are not added multiple\n",
    "#times to the database\n",
    "try:\n",
    "    status = db['tracker']\n",
    "except KeyError:\n",
    "    \"Tracker document not found.\"\n",
    "    \n",
    "final = ch.getResultsfromView(\"final\", \"names\", db)\n",
    "for doc in final.result:\n",
    "    doc = doc['value']\n",
    "    doc = ch.convertToDocument(db, doc['_id'])\n",
    "    doc.fetch()\n",
    "    doc.delete()\n",
    "status['finished_switch'] = 0\n",
    "status['final'] = []\n",
    "status.save()\n",
    "\n",
    "if utils.add_review(status) == 4:\n",
    "    clustered = ch.getResultsfromView(\"clustered\", \"names\", db)\n",
    "    for doc in clustered.result:\n",
    "        doc = doc['value']\n",
    "        name = doc['product_name']\n",
    "        asin = doc['product_id']\n",
    "        if asin not in status['final']:\n",
    "            del doc['_id']\n",
    "            final = {}\n",
    "            final = utils.make_final(db, doc)\n",
    "            final['type'] = 'final'\n",
    "            final['product_id'] = asin\n",
    "            final['product_name'] = name\n",
    "            status['final'].append(asin)\n",
    "            db.create_document(final)\n",
    "            status.save()\n",
    "    status['finished_switch'] = 1\n",
    "    status.save()\n",
    "    \n",
    "client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary\n",
    "\n",
    "At this point you should have been able to:\n",
    "\n",
    "1. Analyze the data you will be using for the application by:\n",
    "    - Replacing words by their semantic types;\n",
    "    - Identifying relationships between entities;\n",
    "    - Identifying the sentiment associated with the entities;\n",
    "2. Classify sentences of your data based on the NLC model trained previously;\n",
    "3. Cluster sentences, along with their overall sentiments, associated with the entity classes you trained on your\n",
    "    linguistic model;\n",
    "4. Deploy the sample app locally pointing to the Cloudant instance you created (if you took this route), or to \n",
    "    use your own developed UI to point to your persisted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
