{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Watson Knowledge Studio](https://www.ibm.com/marketplace/cloud/supervised-machine-learning/us/en-us) is a tool from IBM that allows you to train a natural language model on your own custom domain. Documents from a domain are uploaded to the service and annotated using a custom system of entities and relationships. The end result is a model that can be exported to the Alchemy API.\n",
    "\n",
    "For the Voice of the Customer use case, demonstrated by the demo in this repository, the files for training a costumized Alchemy model through WKS can be found under the ``resources`` folder. In order to reproduce the steps on your own account, follow step 2 of this notebook in order to acquire a WKS account. After you have signed up, follow the steps:\n",
    "    \n",
    "   1. Upload the types.json file under the ``Type System`` > ``Entity Types`` tab.\n",
    "   2. Upload the dictionary.zip file under the ``Dictionaries`` tab.\n",
    "   3. Upload the documents.zip file under the ``Documents`` tab.\n",
    "   4. Create a ``Machine Learning`` annotator under the ``Annotator Component`` tab.\n",
    "   5. Once the annotator has finished training, click on the Detail button and then ``Take Snapshot``.\n",
    "   6. After a Snapshot of the annotator is generated, click on ``Deploy`` and provide your Alchemy API key.\n",
    "    \n",
    "Following the above steps will allow you to replicate the customized Alchemy model created for this use case. In case you want to create a customize Alchemy model for your own domain data, follow the steps below, which detail how to create entity types, relations, ground truth and how to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Making sure you have the data you need\n",
    "\n",
    "The first step for creating your customized natural language model is to make sure you have the data that you will use to train it. You need text that is typical of the data that you are going to be using for this application. For this example, Amazon Product Reviews are used as the data for training the WKS model.\n",
    "\n",
    "Generally, you would want to have as many examples of text as feasible for you to annotate. It is recommended that you have several hundred examples. For this sample application 100 reviews were annotated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Signing up for WKS\n",
    "\n",
    "The next step is to go to Watson Knowledge Studio and create an account. One option os to create a trial account. Click on \"Free 30-Day Trial\" button on the webpage provided on the previous step. After submitting the form to create an account, a service instance will be provisioned. This step usually takes a few minutes, but you should receive an email once it is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Annotation step\n",
    "\n",
    "Once you receive the confirmation email, you are ready for the next step. If you are annotating alone, go ahead and start the service up. If you plan on annotating as a team, you need to expand the option box and use the \"Add new user:\" option. If you want them to only annotate, leave the box unchecked, but if you want a team member to help configure anything about the documents, the type system, or the annotator, check the \"Make administrator\" button.\n",
    "Once you have started the service, click on the url that is presented, it will lead you to the instance of WKS that you just started. From there, you are ready to start your project. Create a new project. You will need to give it a name but you do not need to worry about any of the other options.\n",
    "\n",
    "## Creating entities and relationships\n",
    "\n",
    "From there, you’ll be taken to your project dashboard. The first screen that you see is the type system management screen. Here you can see all of the entities and relationships (none yet) that you have defined for this project. At this point, you need to start getting a little creative. For each type of data that you want to process at a later point, you need to define an entity. For instance, if you want information about the features of a product, information on customer service, and information on the defects people find, so you would create ``Feature``, ``Customer_Service``, and ``Defect`` entities. \n",
    "\n",
    "You will most likely not need to define subtypes or mentions for the entities, but if you would like to get more sophistocated, you can. ``Roles`` are the ways that entities can act like other entities. For instance, a camera is a product but it can also be a feature of something, so if somebody is talking about their phone's camera, it could be a product that has the role of a feature. ``Subtypes`` are the different classes that are a more specific type of another class, ``online customer serv`` is a subtype of ``customer service``. For more information on these, please visit the [WKS documentation](https://www.ibm.com/watson/developercloud/doc/wks/wks_overview.shtml) on Type Systems. \n",
    "\n",
    "## Uploading your data\n",
    "\n",
    "In order to upload the data to be annotated, click the \"Documents\" link in the top bar of the page. This is where you can upload the .csv files containing your data. Click import, find the files and import it into WKS. It seems counter-intuitive, but you cannot use those as they are, your texts need to be divided into document sets. Click the “Create Sets” button, from there you can define everything about the sets you are about to create. ``Overlap`` is the percentage of documents that are shared between each set. If there is more than one annotator assigned to your project, this is used to measure how much agreement there is between the different annotators. If you are annotating by yourself, it may be useful to see how consistent you are, but it is recommended that you set it low so you are not annotating the same thing again and again. We have found that it is almost always better to create lots of little sets, rather than one large one. The machine learning component will not accept documents from sets that aren't completely annotated, so small sets will let you add to the machine learning component more frequently.\n",
    "\n",
    "## Creating dictionaries\n",
    "\n",
    "Now that you have the documents all set up, you can add dictionaries if you want. Dictionaries are not necessary, but if you find yourself annotating the same word over and over again, they can help. Dictionaries will pre-annotate case-sensitive string matches to those in the entry. If you would like to use them, you can create one and add entries to it. The ``surface`` form of a word is the inflected version of a word that may show up, while the ``lemma`` is the base form of the word. \"Running\" is one surface form that can result from the lemma \"Run\".\n",
    "\n",
    "It is important to note that, if you would like to use dictionaries, you need to run the dictionary pre-annotator on your document sets before you start annotating. For more information on that please visit the WKS documentation on Dictionaries.\n",
    "\n",
    "## Annotating documents\n",
    "\n",
    "Now is finally the time to annotate. Follow the \"Human Annotation\" link in the top bar to arrive at the annotation page. From there, you can check on the status of annotation tasks as well as configure hotkeys and some cosmetics of the editor. Click on the \"Add Task\" button and add all of the document sets that you would like to annotate. Once you're all set up, click on one of the document sets, then on one of the documents, to start annotating.\n",
    "\n",
    "Anytime in your text that some word is mentioned that fits into one of the entity types that you defined, click on the word and then click (or press the hotkey of) the type on the side bar. Once you have combed through the entire text picking out entity types, you can annotate relationships (in case you defined any). Select the ``relationships`` view on the left side then click the first entity in the relationship, then the second, then click (or press the hotkey of) the relationship type in the sidebar.\n",
    "\n",
    "Now you can annotate something called ``coreference``. Coreference refers to when there is a word that is actually talking about something else. For instance, in the sentence, \"Joe went to the store because he was out of milk\" both ``Joe`` and ``he`` are corefferents; they refer to the same thing. So, anytime that the exact same thing is mentioned more than once (by different referring expressions or pronouns), you can mark it as a coreference chain. In order to do this, click each word that is referring to the same thing, and then when you're all done double click the last word. There are also options to combine or delete chains on the right side.\n",
    "\n",
    "## Creating the ground truth for training\n",
    "\n",
    "Once you are finished annotating, it is time to accept those to the ``Ground Truth``. Navigate back to the screen where you can check the status of your annotation task and accept the document sets. If there was any overlap, there will most likely be conflicts between those documents. You can use the provided tool to check those by going through and accepting the correct annotations. It is important to be careful on this step. If one document set is accepted, and then another one is at a later time the second will overwrite the first. If you have a high document overlap, that would result in the second set overwriting everything in the first set with no chance to resolve conflicts.\n",
    "\n",
    "After that last step, you should have a ground truth all set up, which means you are ready to let the machine start learning. Navigate to \"Annotator Component\" using the top bar. Click the button to create an annotator and select the option to create the machine learning annotator. Then, select all of the document sets that you have annotated, and define a training/testing split (the default is usually good). Click the option to train and evaluate, then come back in a couple of minutes to see how you did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Publishing your model to Alchemy\n",
    "\n",
    "Go into the detail view of the machine annotator and click on the \"Take Snapshot\" button. It will take a few minutes in order to save the snapshot. When it completes, click \"Deploy\". Take your API key from the Alchemy Language API in Bluemix (which you should have already created under the ``Advanced`` plan), and insert into the correct field. You should now have the model-ID of a machine learning powered natural language processing model that you can plug into Alchemy Language to use instead of the available standard model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
